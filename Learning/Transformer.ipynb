{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce79dd06-5099-420d-92d8-23453be52290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer, Embedding, GRU, Dense, LayerNormalization, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Function to load dataset\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# Load Harry Potter book text\n",
    "file_path = \"/Users/twinklemittal/Desktop/AI/Machine_learning/machine_learning_algorithms/Learning_Dataset/Harry_potter/01 Harry Potter and the Sorcerers Stone.txt\"\n",
    "text = load_data(file_path).lower()\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text into sequences\n",
    "input_sequences = []\n",
    "tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "seq_length = 50  # Each input sequence will have 50 words\n",
    "\n",
    "# First seq_length tokens (input): Used for training the model.\n",
    "# Last token (target): Used as the label the model tries to predict.\n",
    "# so total of (50 + 1) in one input_sequence index\n",
    "\n",
    "for i in range(seq_length, len(tokens)):\n",
    "    input_sequences.append(tokens[i - seq_length:i + 1])\n",
    "\n",
    "# Pad sequences and split into inputs (X) and labels (y)\n",
    "# after this X will have inputs and y will have label for those inputs\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# One-hot encode the labels , note- there are other ways for\n",
    "# encoding like pre-trained word2vec encoding and so on\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)  # One-hot encode labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17ac0adb-cdc7-41ab-8da6-11aaecdc7b64",
   "metadata": {},
   "source": [
    " Core of Transformer Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a09a2e4-869e-4cb7-85fa-f0df22aadf3d",
   "metadata": {},
   "source": [
    "for every head Q, K, V -> dimension = 64, we have 8 heads so combined dimersion = 8 * 64 = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce7a6d0-7d0f-45dc-800c-d0ea1715b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim  # 512\n",
    "        self.num_heads = num_heads  # \n",
    "        # embed_dim = dimension of Q, K, and V before splitting into multiple heads\n",
    "        # It is same as total dimension of the input embeddings (word embeddings)\n",
    "\n",
    "        \n",
    "        self.projection_dim = self.embed_dim // self.num_heads  # 64 # Size of Each Attention Head's Subspace\n",
    "        # Each head gets a smaller subspace of the embedding dimension\n",
    "        # example - 64\n",
    "\n",
    "\n",
    "        # Fully connected (dense) layers that project the input into Q,K,V\n",
    "        # These layers map the input embeddings to the same embed_dim\n",
    "        # These layers will be reshaped / split later to split across attention heads\n",
    "        # A single large matrix multiplication is more efficient than many small ones\n",
    "        # GPUs love large matrix multiplications because they are optimized for parallel computation\n",
    "        # This allows TF/Keras to efficiently batch the computation, leveraging better GPU memory utilization\n",
    "\n",
    "        self.query_dense = Dense(self.embed_dim)   # Q Determines \"what to focus on\"\n",
    "        self.key_dense = Dense(self.embed_dim)  # K Acts as \"labels\" to be matched with queries\n",
    "        self.value_dense = Dense(self.embed_dim) # V Holds the actual information\n",
    "\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "        # After multi-head attention is applied, the outputs from all heads are concatenated back into embed_dim\n",
    "        \n",
    "                        \n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        scores = tf.matmul(query, key, transpose_b=True)  # Q * K Transpose\n",
    "        scores /= tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) # Converting int to float32\n",
    "        attention_probs = tf.nn.softmax(scores, axis=-1) # how much attention each token should give to other tokens\n",
    "        # The higher the score, the more focus that token gets\n",
    "        # Softmax should be applied along the keys (i.e., across the last dimension of the scores matrix)\n",
    "        # Each row corresponds to a query token attending to all key tokens\n",
    "        # This ensures that each query distributes its attention across all keys properly\n",
    "        # Each row sums to 1\n",
    "        return tf.matmul(attention_probs, value), attention_probs # Probs * V\n",
    "\n",
    "    # x - query, key or value with shape - (batch_size, seq_len, embed_dim)\n",
    "    # batch_size - number of sequences being processed in parallel (for batch processing)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0,2,1,3])\n",
    "        # before transpose - (batch_size, seq_len, num_heads, projection_dim)\n",
    "        # after transpose - (batch_size, num_heads, seq_len, projection_dim)\n",
    "        # The -1 in tf.reshape is a placeholder that tells TensorFlow to automatically\n",
    "        # infer that dimension's value based on the total number of elements in the tensor\n",
    "        # -1 is replaced by seq_len by tensorflow\n",
    "\n",
    "    # In TF,Keras - call(self, inputs) is a standard method used inside Layer subclasses\n",
    "    # to define the forward pass of a neural network layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value = inputs\n",
    "        batch_size = tf.shape(query)[0]  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        query = self.split_heads(self.query_dense(query), batch_size)\n",
    "        key = self.split_heads(self.key_dense(key), batch_size)\n",
    "        value = self.split_heads(self.value_dense(value), batch_size)\n",
    "\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # before transpose - (batch_size, num_heads, seq_len, projection_dim)\n",
    "        # after transpose -  (batch_size, seq_len, num_heads, projection_dim)\n",
    "\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        # Merges all heads back into a single vector\n",
    "        # (batch_size, seq_len, num_heads, projection_dim) → (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return self.combine_heads(concat_attention)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        # y = (x - mean) / root(variance + epsilon)\n",
    "        # epsilon ensures we never divide by zero\n",
    "        # it is small enough not to affect the result but large enough to prevent instability\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att([inputs, inputs, inputs])\n",
    "\n",
    "        # Dropout randomly deactivates some neurons during training to reduce overfitting\n",
    "        # Ensure dropout is only applied during training, not inference\n",
    "\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Residual Connection\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)   # Residual Connection\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        # The Embedding layer takes an integer tensor and replaces each integer with an embed_dim-sized vector\n",
    "        # example - positions = [0, 1, 2, 3]\n",
    "        # after embedding - positions = [\n",
    "        #   [0.2, 0.1, 0.3, 0.5, 0.6, 0.9, 0.7, 0.8],  # Position 0\n",
    "        #   [0.4, 0.2, 0.1, 0.6, 0.5, 0.7, 0.9, 0.3],  # Position 1\n",
    "        #   [0.5, 0.3, 0.8, 0.2, 0.7, 0.4, 0.6, 0.1],  # Position 2\n",
    "        #   [0.9, 0.6, 0.2, 0.3, 0.1, 0.8, 0.4, 0.7]   # Position 3\n",
    "        # ]\n",
    "\n",
    "        # initial shape of x - (batch_size, seq_len)\n",
    "        # batch_size: Number of sentences in a batch\n",
    "        # seq_len: Number of tokens (words) in each sentence\n",
    "        # Each value in x is an integer index from 0 to vocab_size - 1\n",
    "        # after embedding - (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # example - embed_dim = 8, batch_size = 2\n",
    "        #     x = [\n",
    "        #   [ [0.2, 0.1, 0.4, 0.3, 0.8, 0.7, 0.6, 0.9],  # Token 2\n",
    "        #     [0.5, 0.3, 0.9, 0.1, 0.2, 0.6, 0.8, 0.7],  # Token 5\n",
    "        #     [0.4, 0.9, 0.2, 0.3, 0.1, 0.7, 0.5, 0.6],  # Token 1\n",
    "        #     [0.3, 0.8, 0.6, 0.2, 0.5, 0.9, 0.7, 0.4]   # Token 7\n",
    "        #   ],  # First sentence\n",
    "\n",
    "        #   [ [0.1, 0.6, 0.9, 0.7, 0.3, 0.5, 0.2, 0.8],  # Token 0\n",
    "        #     [0.4, 0.2, 0.3, 0.9, 0.7, 0.5, 0.1, 0.6],  # Token 3\n",
    "        #     [0.8, 0.5, 0.4, 0.1, 0.6, 0.3, 0.2, 0.7],  # Token 8\n",
    "        #     [0.9, 0.3, 0.5, 0.7, 0.8, 0.2, 0.6, 0.1]   # Token 4\n",
    "        #   ]   # Second sentence\n",
    "        # ]        \n",
    "        \n",
    "    \n",
    "    def call(self, x):\n",
    "        # the maximum sequence length the model can handle\n",
    "        maxlen = tf.shape(x)[-1] # sets maxlen to the length of the input sequence\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1) # Generate [0, 1, 2, ..., maxlen-1]\n",
    "        positions = self.pos_emb(positions) # Each position index is mapped to a trainable embedding of shape (maxlen, embed_dim)\n",
    "        x = self.token_emb(x) # Each token ID in x is mapped to an embedding of shape (batch_size, maxlen, embed_dim)\n",
    "        return x + positions\n",
    "\n",
    "        # x has shape (batch_size, seq_len, embed_dim)\n",
    "        # positions has shape (maxlen, embed_dim)\n",
    "        # But maxlen == seq_len, so positions effectively has shape (seq_len, embed_dim).\n",
    "        # TensorFlow broadcasts positions across batch_size, treating it as if it were (1, seq_len, embed_dim).\n",
    "        # This allows element-wise addition between x and position\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b401de02-ae24-4c1f-934d-406f0b81e493",
   "metadata": {},
   "source": [
    "Model the whole architecture , compile and run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2939a49a-d915-4c94-ab86-c35e1b04a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50, 128)\n",
      "(None, 50, 128)\n",
      "(None, 128)\n",
      "(None, 6663)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">859,264</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6663</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">859,527</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m859,264\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m198,272\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ get_item (\u001b[38;5;33mGetItem\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6663\u001b[0m)           │       \u001b[38;5;34m859,527\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Parameters\n",
    "embed_dim = 128  # Embedding size\n",
    "num_heads = 4    # Number of attention heads\n",
    "ff_dim = 512     # Feed-forward layer size\n",
    "maxlen = seq_length # here it is 50 defined above\n",
    "\n",
    "# below total words = 6662 (see above - basically all tokens in the text)\n",
    "\n",
    "# Build the model\n",
    "inputs = tf.keras.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, total_words, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "print(x.shape)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x, training=True)\n",
    "print(x.shape)\n",
    "x = x[:, -1, :]\n",
    "print(x.shape)\n",
    "x = Dense(total_words, activation=\"softmax\")(x)\n",
    "print(x.shape)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b12366-49d3-453a-8a44-c0108ceca5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 34ms/step - accuracy: 0.1046 - loss: 6.1419\n",
      "Epoch 2/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 35ms/step - accuracy: 0.1581 - loss: 5.1089\n",
      "Epoch 3/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 35ms/step - accuracy: 0.1977 - loss: 4.3928\n",
      "Epoch 4/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 36ms/step - accuracy: 0.2409 - loss: 3.8071\n",
      "Epoch 5/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 37ms/step - accuracy: 0.2904 - loss: 3.3014\n",
      "Epoch 6/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 36ms/step - accuracy: 0.3585 - loss: 2.8329\n",
      "Epoch 7/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 36ms/step - accuracy: 0.4299 - loss: 2.4137\n",
      "Epoch 8/10\n",
      "\u001b[1m 939/2531\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 36ms/step - accuracy: 0.5907 - loss: 1.6469"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bfc18-a950-4a58-9588-0b65f021471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"harry looked at\"\n",
    "generated_text = generate_text(seed_text, next_words=50, max_sequence_len=seq_length + 1)\n",
    "print(len(generated_text))\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6090f11c-34ef-41ce-9260-843e862b6f31",
   "metadata": {},
   "source": [
    "What Is Missing Compared to ChatGPT?\n",
    "\n",
    "Masked Attention:\n",
    "ChatGPT uses causal masking so that a word cannot see future words during training. Our model uses regular attention, which allows it to see the entire sequence.\n",
    "\n",
    "Multiple Stacked Transformer Blocks:\n",
    "ChatGPT has many layers (e.g., 12, 24, 96 layers). Our model has only one Transformer block.\n",
    "\n",
    "Tokenization & Byte-Pair Encoding (BPE):\n",
    "ChatGPT does not use simple tokenization; it uses Byte-Pair Encoding (BPE) or WordPiece for better vocabulary handling. Our model uses basic word tokenization.\n",
    "\n",
    "Training on Large Datasets:\n",
    "ChatGPT is trained on hundreds of GBs of text. Our model is trained on a single Harry Potter book (very limited).\n",
    "\n",
    "Decoding Strategies for Text Generation:\n",
    "ChatGPT uses sampling (top-k, nucleus sampling) or beam search to generate text. Our model does not have a decoding strategy.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
