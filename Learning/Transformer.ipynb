{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce79dd06-5099-420d-92d8-23453be52290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Function to load dataset\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# Load Harry Potter book text\n",
    "file_path = \"/Users/twinklemittal/Desktop/AI/Machine_learning/machine_learning_algorithms/Learning_Dataset/Harry_potter/01 Harry Potter and the Sorcerers Stone.txt\"\n",
    "text = load_data(file_path).lower()\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text into sequences\n",
    "input_sequences = []\n",
    "tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "seq_length = 50  # Each input sequence will have 50 words\n",
    "\n",
    "for i in range(seq_length, len(tokens)):\n",
    "    input_sequences.append(tokens[i - seq_length:i + 1])\n",
    "\n",
    "# Pad sequences and split into inputs (X) and labels (y)\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)  # One-hot encode labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17ac0adb-cdc7-41ab-8da6-11aaecdc7b64",
   "metadata": {},
   "source": [
    "Transformer Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a09a2e4-869e-4cb7-85fa-f0df22aadf3d",
   "metadata": {},
   "source": [
    "for every head Q, K, V -> dimension = 64, we have 8 heads so combined dimersion = 8 * 64 = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce7a6d0-7d0f-45dc-800c-d0ea1715b842",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (3926835580.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef __init__(self, embed_dim, num_heads):\u001b[39m\n                                             ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim  # 512\n",
    "        self.num_heads = num_heads  # 8\n",
    "        self.projection_dim = self.embed_dim // self.num_heads  # 64\n",
    "\n",
    "        self.query_dense = Dense(self.embed_dim)\n",
    "        self.key_dense = Dense(self.embed_dim)\n",
    "        self.value_dense = Dense(self.embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        scores = tf.matmul(query, key, transpose_b=True)  # Q * K Transpose\n",
    "        scores /= tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) # Converting int to float32\n",
    "        attention_probs = tf.nn.softmax(scores, axis=-1) # sum along a row to 1\n",
    "        return tf.matmul(attention_probs, value) # Probs * V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98d8d3-ce0d-44b1-994a-e2e5a9459c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
