{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dc9c70-90c8-45c7-9d17-ad8e7ab0c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "raw",
   "id": "823003b3-7556-497d-b6ff-7c3eafe9985b",
   "metadata": {},
   "source": [
    "Fine-tuning is not just for domain specialization but also for enhancing instruction-following capabilities. Since Meta-Llama-3.1-8B is a base model, it has knowledge but lacks strong instruction-following abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c7bbef-e6d4-4142-b877-29f75935f26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: accelerate in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (1.10.1)\n",
      "Collecting xformers==0.0.29.post3\n",
      "  Downloading xformers-0.0.29.post3.tar.gz (8.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m275.7 kB/s\u001b[0m  \u001b[33m0:00:29\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: peft in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (0.17.1)\n",
      "Collecting trl\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement triton (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for triton\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7491ad47-c526-47b6-94d0-657a0d7591d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: protobuf in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (6.32.0)\n",
      "Requirement already satisfied: datasets in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (0.34.4)\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-macosx_11_0_arm64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from huggingface_hub) (1.1.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/twinklemittal/.venvs/nlp312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m227.8 kB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m127.2 kB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, hf_transfer\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [hf_transfer]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf_transfer-0.1.9 sentencepiece-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece protobuf datasets huggingface_hub hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e85932-cf80-4ec2-bb6f-a737f66ea6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.9.4-py3-none-any.whl.metadata (52 kB)\n",
      "Downloading unsloth-2025.9.4-py3-none-any.whl (313 kB)\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2025.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd0bebd-3dcf-4a4d-b67d-8d7025c251fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth currently only works on NVIDIA GPUs and Intel GPUs.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# FastLanguageModel is a specialized, highly optimized wrapper around Hugging Face Transformers\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# It is designed for Efficient fine-tuning, Seamless integration with LoRA, QLoRA, and 4-bit quantization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# maximum sequence length - how many tokens that model can handle at once\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/nlp312/lib/python3.12/site-packages/unsloth/__init__.py:79\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m DEVICE_TYPE : \u001b[38;5;28mstr\u001b[39m = \u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_count\u001b[39m():\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m DEVICE_TYPE == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/nlp312/lib/python3.12/site-packages/unsloth/__init__.py:77\u001b[39m, in \u001b[36mget_device_type\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m torch.xpu.is_available():\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."
     ]
    }
   ],
   "source": [
    "# FastLanguageModel is a specialized, highly optimized wrapper around Hugging Face Transformers\n",
    "# It is designed for Efficient fine-tuning, Seamless integration with LoRA, QLoRA, and 4-bit quantization\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# maximum sequence length - how many tokens that model can handle at once\n",
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detetcion, Float16 for Tesla T4, V100, Bfloat16 fo Amper+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e4269d-85a9-420d-86a3-55bbb19a4e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "MPS available: True\n",
      "XPU available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())  # Apple Silicon\n",
    "print(\"XPU available:\", hasattr(torch, \"xpu\") and torch.xpu.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496387aa-c9c9-4f4f-8ff2-b0ddfae904a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (nlp312)",
   "language": "python",
   "name": "nlp312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
